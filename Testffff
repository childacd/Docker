从你提供的错误信息来看，问题的核心是 Spark 无法找到 Avro 文件的读取库。这通常是因为 Spark 的 Avro 数据源 没有正确配置，或者没有在 PySpark 环境中安装 Avro 支持。

错误的根本原因

错误信息指出：

Failed to find the required source: avro. Avro is built-in but external data source module since Spark 2.4.

这意味着 Spark 在版本 2.4 及以后，Avro 已经作为外部数据源模块被分离出来了。你需要手动添加 Avro 支持。

解决方案：
	1.	安装 PySpark 的 avro 支持
Spark 需要 Avro 数据源 来读取 .avro 格式的文件。在 Spark 2.4 及以后版本中，Avro 被作为外部模块分离出来，所以你需要安装相关的依赖。
	2.	通过 Maven 包添加 Avro 支持
你需要在创建 SparkSession 时添加 Avro 数据源的依赖：

from pyspark.sql import SparkSession

# 创建 SparkSession 并添加 Avro 数据源依赖
spark = SparkSession.builder \
    .appName("Avro to Parquet") \
    .config("spark.jars.packages", "org.apache.spark:spark-avro_2.12:3.1.2") \
    .getOrCreate()

# 读取 Avro 文件
df = spark.read.format("avro").load("path_to_input.avro")

# 将数据写入 Parquet 文件
df.write.parquet("path_to_output.parquet")

print("Avro file has been successfully converted to Parquet.")

这里的 spark.jars.packages 指定了需要添加的依赖包，org.apache.spark:spark-avro_2.12:3.1.2 是适用于 Spark 3.x 版本的 Avro 数据源依赖。如果你使用的是不同版本的 Spark，可以调整相应的依赖版本。

	3.	确保依赖正确安装
如果你在本地运行 pyspark 并没有自动下载依赖包，可以手动下载并提供给 Spark：

pip install pyspark[avro]

这个命令会确保在 pyspark 中安装 Avro 支持。

	4.	确保你提供的路径是正确的
确保你提供的路径是正确的。在本地路径中，使用相对路径或者绝对路径。你可以直接使用当前目录下的文件，例如：

df = spark.read.format("avro").load("file:///" + "/path/to/your/avro_file.avro")



总结
	•	Avro 依赖：你需要显式添加 Spark 的 avro 依赖，可以通过 spark.jars.packages 配置来加载它。
	•	路径问题：确保提供的 Avro 文件路径正确。
	•	安装 Avro 支持：确保 pyspark 配置了 Avro 支持。

参考：
	•	你可以根据 Spark 版本和 PySpark 版本，选择合适的依赖版本。例如，Spark 3.x 使用 spark-avro_2.12 版本。
	•	在 PySpark 中，你也可以直接使用 pip install pyspark[avro] 来安装所需的依赖。

如果仍然遇到问题或需要进一步帮助，请随时告诉我！
