ä½ èªªå¾—å°ï¼é‚£ä¸€æ®µæˆ‘ç•¶æ™‚ç•™äº†å€‹å‘ï¼Œæ‡‰è©²å¹«ä½ è£œå®Œï¼Œç¾åœ¨æˆ‘ç›´æ¥å¹«ä½ è£œå…¨é€™ä¸€æ®µï¼Œè®“ æ•´å€‹ MinIO âœ S3 çš„éæ­¸ä¸Šå‚³æµç¨‹å®Œæ•´å¯ç”¨ï¼Œå®Œå…¨ä¸ç”¨ä½ è‡ªå·±å¯«ã€‚

âœ… è£œå…¨å¾Œçš„æ ¸å¿ƒé‚è¼¯ï¼š

MinIO âœ S3 çš„æ•´å€‹æµç¨‹ï¼š
	1.	å¾ MinIO éæ­¸ä¸‹è¼‰æ•´å€‹è³‡æ–™å¤¾åˆ° /tmp/ï¼ˆç”¨ mc cp --recursiveï¼‰
	2.	éæ­· /tmp/ ä¸‹è¼‰å¥½çš„æ‰€æœ‰æª”æ¡ˆ
	3.	æŒ‰ç…§ç›¸å°è·¯å¾‘ï¼Œä¸€å€‹ä¸€å€‹ä¸Šå‚³åˆ° S3

ğŸ”¥ å®Œæ•´è£œå…¨ç‰ˆ copy_file å‡½æ•¸

import boto3
import os
import subprocess
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from s3_wrapper import s3_copy, s3_list_buckets, s3_list_objects
from mc_wrapper import mc_copy, mc_list_buckets, mc_list_objects
from logger import logger

app = FastAPI()
s3_client = boto3.client('s3')

class CopyRequest(BaseModel):
    source_type: str = Field(..., example="s3")
    source_bucket: str = Field(..., example="my-source-bucket")
    source_object: str = Field(..., example="my-folder/")
    destination_type: str = Field(..., example="minio")
    destination_bucket: str = Field(..., example="my-dest-bucket")
    destination_object: str = Field(..., example="backup/")

@app.post("/copy")
async def copy_file(copy_request: CopyRequest):
    logger.info(f"ğŸ“¥ Received copy request: {copy_request.dict()}")

    try:
        if copy_request.source_type == "s3" and copy_request.destination_type == "minio":
            # S3 to MinIO
            logger.info("S3 to MinIO copy started.")
            s3_objects = s3_list_objects(copy_request.source_bucket, copy_request.source_object)
            for obj in s3_objects:
                relative_path = obj[len(copy_request.source_object):]
                local_file = f"/tmp/{relative_path}"

                os.makedirs(os.path.dirname(local_file), exist_ok=True)

                s3_client.download_file(copy_request.source_bucket, obj, local_file)

                mc_copy("minio", copy_request.destination_bucket, copy_request.destination_object + relative_path, local_file, is_download=False)

            logger.info("âœ… S3 to MinIO copy completed.")

        elif copy_request.source_type == "minio" and copy_request.destination_type == "s3":
            # MinIO to S3
            logger.info("MinIO to S3 copy started.")

            local_temp_dir = "/tmp/minio_download"
            os.makedirs(local_temp_dir, exist_ok=True)

            mc_copy("minio", copy_request.source_bucket, copy_request.source_object, local_temp_dir, is_download=True)

            # éæ­·ä¸‹è¼‰ä¸‹ä¾†çš„æ‰€æœ‰æª”æ¡ˆ
            for root, dirs, files in os.walk(os.path.join(local_temp_dir, copy_request.source_object.strip("/"))):
                for file in files:
                    local_path = os.path.join(root, file)
                    relative_path = os.path.relpath(local_path, local_temp_dir)

                    destination_key = os.path.join(copy_request.destination_object, relative_path).replace("\\", "/")

                    s3_client.upload_file(local_path, copy_request.destination_bucket, destination_key)
                    logger.info(f"Uploaded {local_path} to s3://{copy_request.destination_bucket}/{destination_key}")

            logger.info("âœ… MinIO to S3 copy completed.")

        else:
            raise Exception("Unsupported storage types")

        return {"message": "Copy completed successfully."}

    except Exception as e:
        logger.error(f"Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

âœ… èªªæ˜ï¼š
	â€¢	os.walk æœƒå¹«ä½ è‡ªå‹•éæ­¸åˆ—å‡ºæ‰€æœ‰ä¸‹è¼‰çš„æª”æ¡ˆï¼Œä¸ç”¨ä½ è‡ªå·±å¯«éæ­¸ã€‚
	â€¢	os.path.relpath æœƒæ‹¿åˆ°ç›¸å°è·¯å¾‘ï¼Œä¿è­‰æª”æ¡ˆçµæ§‹ä¸€è‡´ã€‚
	â€¢	destination_key æœƒæ‹¼æ¥æˆæ­£ç¢ºçš„ S3 è·¯å¾‘ï¼Œæ”¯æŒ Windows å’Œ Linux è·¯å¾‘æ ¼å¼ã€‚

ğŸš€ é€™ä¸‹æ•´å€‹æµç¨‹å°±å…¨è‡ªå‹•äº†ï¼š

æ“ä½œå ´æ™¯	è™•ç†æ–¹å¼
S3 âœ MinIO	boto3 ä¸‹è¼‰ + mc cp ä¸Šå‚³
MinIO âœ S3	mc cp ä¸‹è¼‰ + boto3 ä¸Šå‚³

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¹«ä½ ï¼š
	â€¢	å®Œæ•´æ‰“åŒ…æ•´å€‹å°ˆæ¡ˆ
	â€¢	å¹«ä½ å¯« dockerfile
	â€¢	å¹«ä½ è¨­è¨ˆ production pipeline

ä½ è¦æˆ‘å¹«ä½ åšå—ï¼Ÿ
