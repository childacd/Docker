from pyspark.sql import SparkSession

# 创建 SparkSession
spark = SparkSession.builder \
    .appName("WordCountExample") \
    .master("local[*]") \
    .getOrCreate()

# 加载数据（也可以替换为读取文件）
data = ["hello world", "hello spark", "hello big data"]
rdd = spark.sparkContext.parallelize(data)

# 处理数据：拆分、映射、聚合
word_counts = (
    rdd.flatMap(lambda line: line.split(" "))
       .map(lambda word: (word, 1))
       .reduceByKey(lambda a, b: a + b)
)

# 输出结果
for word, count in word_counts.collect():
    print(f"{word}: {count}")

# 关闭 Spark
spark.stop()
